<p align=center>`Vision Transformer`</p>

## Improvement

### Long-Range

**LambdaNetworks: Modeling Long-Range Interactions Without Attention.**<br>
*Irwan Bello.*<br>
ICLR 2021. [[PDF](https://arxiv.org/abs/2102.08602)]

**Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention.**<br>
*Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.*<br>
AAAI 2021. [[PDF](https://arxiv.org/abs/2102.03902v1)] [[Github](https://github.com/mlpen/Nystromformer)]

### Postion Encoding

**Relative Positional Encoding for Transformers with Linear Complexity.**<br>
*Antoine Liutkus, Ondřej Cífka, Shih-Lun Wu, Umut Simsekli, Yi-Hsuan Yang, Gaël RICHARD.*<br>
ICML 2021. [[PDF]()]

**DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification.**<br>
*Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, Cho-Jui Hsieh.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.02034)] [[Project](https://dynamicvit.ivg-research.xyz/)]

**LocalViT: Bringing Locality to Vision Transformers.**<br>
*Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, Luc Van Gool.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.05707)]

**Do We Really Need Explicit Position Encodings for Vision Transformers?**<br>
*Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, Huaxia Xia.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.10882)]

### Accelerate

**Visual Transformer Pruning.**<br>
*Mingjian Zhu, Kai Han, Yehui Tang, Yunhe Wang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.08500)]

**LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference.**<br>
*Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.01136)]

**Transformer in Transformer.**<br>
*Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.00112)] [[Github](https://github.com/huawei-noah/noah-research/tree/master/TNT)]

**Optimizing Inference Performance of Transformers on CPUs.**<br>
*Dave Dice, Alex Kogan.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.06621)]

### Training

**Efficient Training of Visual Transformers with Small-Size Datasets.**<br>
*Yahui Liu, Enver Sangineto, Wei Bi, Nicu Sebe, Bruno Lepri, Marco De Nadai.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.03746)]

**An Empirical Study of Training Self-Supervised Visual Transformers.**<br>
*Xinlei Chen, Saining Xie, Kaiming He.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.02057)]

**SiT: Self-supervised vIsion Transformer.**<br>
*Sara Atito, Muhammad Awais, Josef Kittler.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.03602)]

### Others

**Combiner: Full Attention Transformer with Sparse Computation Cost.**<br> 
*Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, Bo Dai.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/pdf/2107.05768.pdf)]

**ViTGAN: Training GANs with Vision Transformers.**<br> 
*Kwonjoon Lee, Huiwen Chang, Lu Jiang, Han Zhang, Zhuowen Tu, Ce Liu.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/abs/2107.04589)]

**GLiT: Neural Architecture Search for Global and Local Image Transformer.**<br> 
*Boyu Chen, Peixia Li, Chuming Li, Baopu Li, Lei Bai, Chen Lin, Ming Sun, Junjie yan, Wanli Ouyang.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/abs/2107.02960)]

**SVEA: Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation.**<br> 
*Nicklas Hansen, Hao Su, Xiaolong Wang.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/abs/2107.00644)] [[Project](https://nicklashansen.github.io/SVEA)]

**AutoFormer: Searching Transformers for Visual Recognition.**<br> 
*Minghao Chen, Houwen Peng, Jianlong Fu, Haibin Ling.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/abs/2107.00651)] [[Github](https://github.com/microsoft/AutoML)]

**Early Convolutions Help Transformers See Better.**<br> 
*Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollár, Ross Girshick.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.14881)]

**IA-RED2: Interpretability-Aware Redundancy Reduction for Vision Transformers.**<br> 
*Bowen Pan, Yifan Jiang, Rameswar Panda, Zhangyang Wang, Rogerio Feris, Aude Oliva.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.12620)]

**CAT: Cross Attention in Vision Transformer.**<br> 
*Hezheng Lin, Xing Cheng, Xiangyu Wu, Fan Yang, Dong Shen, Zhongyuan Wang, Qing Song, Wei Yuan.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.05786)]

**On Improving Adversarial Transferability of Vision Transformers.**<br> 
*Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Fahad Shahbaz Khan, Fatih Porikli.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.04169)] [[Github](https://git.io/JZmG3)]

**Chasing Sparsity in Vision Transformers:An End-to-End Exploration.**<br> 
*Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, Zhangyang Wang.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.04533)]

**Scaling Vision Transformers.**<br> 
*Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, Lucas Beyer.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.04560)]

**RegionViT: Regional-to-Local Attention for Vision Transformers.**<br> 
*Chun-Fu Chen, Rameswar Panda, Quanfu Fan.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.02689)]

**Patch Slimming for Efficient Vision Transformers.**<br> 
*Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chao Xu, Dacheng Tao.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.02852)]

**Transformer in Convolutional Neural Networks.**<br> 
*Yun Liu, Guolei Sun, Yu Qiu, Le Zhang, Ajad Chhatkuli, Luc Van Gool.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.03180)]

**Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer.**<br> 
*Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, Bin Fu.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.03650)]

**Refiner: Refining Self-attention for Vision Transformers.**<br> 
*Daquan Zhou, Yujun Shi, Bingyi Kang, Weihao Yu, Zihang Jiang, Yuan Li, Xiaojie Jin, Qibin Hou, Jiashi Feng.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.03714)]

**ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision.**<br> 
*Wonjae Kim, Bokyung Son, Ildoo Kim.*<br>
ICML 2021. [[PDF]()]

**MSA Transformer.**<br> 
*Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, Alexander Rives.*<br>
ICML 2021. [[PDF]()]

**Thinking Like Transformers.**<br> 
*Gail Weiss, Yoav Goldberg, Eran Yahav.*<br>
ICML 2021. [[PDF]()]

**Generative Adversarial Transformers.**<br> 
*Dor Arad, Larry Zitnick.*<br>
ICML 2021. [[PDF]()]

**PipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers.**<br> 
*Chaoyang He, Shen Li, Mahdi Soltanolkotabi, Salman Avestimehr.*<br>
ICML 2021. [[PDF]()]

**LieTransformer: Equivariant Self-Attention for Lie Groups.**<br> 
*Michael Hutchinson, Charline Le Lan, Sheheryar Zaidi, Emilien Dupont, Yee Whye Teh, Hyunjik Kim.*<br>
ICML 2021. [[PDF]()]

**PixelTransformer: Sample Conditioned Signal Generation.**<br> 
*Shubham Tulsiani, Abhinav Gupta.*<br>
ICML 2021. [[PDF]()]

**CATE: Computation-aware Neural Architecture Encoding with Transformers.**<br> 
*Shen Yan, Kaiqiang Song, Fei Liu, Mi Zhang.*<br> 
ICML 2021. [[PDF]()]

**TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer.**<br> 
*Berkay Berabi, Jingxuan He, Veselin Raychev, Martin Vechev.*<br>
ICML 2021. [[PDF]()]

**Differentiable Spatial Planning using Transformers.**<br> 
*Devendra Singh Chaplot, Deepak Pathak, Jitendra Malik.*<br>
ICML 2021. [[PDF]()]

**Catformer: Designing Stable Transformers via Sensitivity Analysis.**<br>
*Jared Quincy Davis, Albert Gu, Tri Dao, Krzysztof Choromanski, Christopher Re, Percy Liang, Chelsea Finn.*<br>
ICML 2021. [[PDF]()]

**When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations.**<br>
*Xiangning Chen, Cho-Jui Hsieh, Boqing Gong.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.01548)]

**An Attention Free Transformer.**<br>
*Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, Josh Susskind.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2105.14103)]

## Survey

**A Survey of Transformers.**<br>
*Tianyang Lin, Yuxin Wang, Xiangyang Liu, Xipeng Qiu.*<br>
arxiv 2021. [[PDF](https://arxiv.org/pdf/2106.04554.pdf)]

**A Survey on Visual Transformer.**<br>
*Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, Dacheng Tao.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2012.12556)]

**Transformers in Vision: A Survey.**<br>
*Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, Mubarak Shah.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.01169)]

## Recognition and Classification

**Diverse Part Discovery: Occluded Person Re-identification with Part-Aware Transformer.**<br>
*Yulin Li, Jianfeng He, Tianzhu Zhang, Xiang Liu, Yongdong Zhang, Feng Wu.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2106.04095)]

**Person Re-Identification with a Locally Aware Transformer.**<br>
*Charu Sharma, Siddhant R. Kapil, David Chapman.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.03720)]

**Glance-and-Gaze Vision Transformer.**<br>
*Qihang Yu, Yingda Xia, Yutong Bai, Yongyi Lu, Alan Yuille, Wei Shen.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.02277)] [[Github](https://github.com/yucornetto/GG-Transformer)]

**Visformer: The Vision-friendly Transformer.**<br>
*Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, Qi Tian.*<br>
arxiv 2021. [[PDF](https://arxiv.org/pdf/2104.12533.pdf)] [[Github](https://github.com/danczs/Visformer)]

**Multiscale Vision Transformers.**<br>
*Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, Christoph Feichtenhofer.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.11227)] [[Github](https://github.com/facebookresearch/SlowFast)]

**Face Transformer for Recognition.**<br>
*Yaoyao Zhong, Weihong Deng.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.14803)]

**DeepViT: Towards Deeper Vision Transformer.**<br>
*Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou, Jiashi Fengv.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.11886)]

**ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases.**<br>
*Stéphane d'Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, Levent Sagun.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.10697)]

**TimeSformer: Is Space-Time Attention All You Need for Video Understanding?**<br>
*Gedas Bertasius, Heng Wang, Lorenzo Torresani.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.05095)] [[Github](https://github.com/lucidrains/TimeSformer-pytorch)]

**MIST: Multiple Instance Spatial Transformer Network.**<br>
*Baptiste Angles, Yuhe Jin, Simon Kornblith, Andrea Tagliasacchi, Kwang Moo Yi.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/1811.10725)]

**OmniNet: Omnidirectional Representations from Transformers.**<br>
*Yi Tay, Mostafa Dehghani, Vamsi Aribandi, Jai Gupta, Philip Pham, Zhen Qin, Dara Bahri, Da-Cheng Juan, Donald Metzler.*<br>
arxiv 2021. [[PDF](https://arxiv.org/pdf/2103.01075.pdf)]

**Relaxed Transformer Decoders for Direct Action Proposal Generation.**<br>
*Jing Tan, Jiaqi Tang, Limin Wang, Gangshan Wu.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.01894)]

**Video Transformer Network.**<br>
*Daniel Neimark, Omri Bar, Maya Zohar, Dotan Asselmann.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.00719)

**Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.**<br>
*Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, Shuicheng Yan.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.11986)] [[Github](https://github.com/yitu-opensource/T2T-ViT)]

**Vision Transformer-An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.**<br>
*Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.*<br>
ICLR 2021. [[PDF](https://arxiv.org/abs/2010.11929)] [[Official](https://github.com/google-research/vision_transformer)] [[Github](https://github.com/lukemelas/PyTorch-Pretrained-ViT)] [[Github](https://github.com/lucidrains/vit-pytorch)] [[Github](https://github.com/gupta-abhay/ViT)] [[Code Collection](https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers)]

**Temporal-Relational CrossTransformers for Few-Shot Action Recognition.**<br>
*Toby Perrett, Alessandro Masullo, Tilo Burghardt, Majid Mirmehdi, Dima Damen.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.06184)]

**Bottleneck Transformers for Visual Recognition.**<br>
*Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, Ashish Vaswani.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.11605)]

## Tracking, Detection and Segmentation

**Long Short-Term Transformer for Online Action Detection.**<br>
*Mingze Xu, Yuanjun Xiong, Hao Chen, Xinyu Li, Wei Xia, Zhuowen Tu, Stefano Soatto.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2107.03377)]

**Video Swin Transformer.**<br>
*Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, Han Hu.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.13230)]

**Video Instance Segmentation using Inter-Frame Communication Transformers.**<br>
*Sukjun Hwang, Miran Heo, Seoung Wug Oh, Seon Joo Kim.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.03299)]

**Few-Shot Segmentation via Cycle-Consistent Transformer.**<br>
*Gengwei Zhang, Guoliang Kang, Yunchao Wei, Yi Yang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.02638)]

**Associating Objects with Transformers for Video Object Segmentation.**<br>
*Zongxin Yang, Yunchao Wei, Yi Yang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.02638)]

**You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection.**<br>
*Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.00666)]

**TransVOS: Video Object Segmentation with Transformers.**<br>
*Jianbiao Mei, Mengmeng Wang, Yeneng Lin, Yong Liu.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.00588)]

**SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers.**<br>
*Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2105.15203)]

**Multi-Modal Fusion Transformer for End-to-End Autonomous Driving.**<br>
*Aditya Prakash, Kashyap Chitta, Andreas Geiger.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2104.09224)]

**HOTR: End-to-End Human-Object Interaction Detection with Transformers.**<br>
*Bumsoo Kim, Junhyun Lee, Jaewoo Kang, Eun-Sol Kim, Hyunwoo J. Kim.*<br>
CVPR 2021 (oral). [[PDF](https://arxiv.org/abs/2104.13682)]

**Twins: Revisiting Spatial Attention Design in Vision Transformers.**<br>
*Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, Chunhua Shen.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.13840)]

**Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.**<br>
*Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.14030)] [[Github](https://github.com/microsoft/Swin-Transformer)]

**Looking Beyond Two Frames: End-to-End Multi-Object Tracking Using Spatial and Temporal Transformers.**<br>
*Tianyu Zhu, Markus Hiller, Mahsa Ehsanpour, Rongkai Ma, Tom Drummond, Hamid Rezatofighi.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.14829)]

**TransCenter: Transformers with Dense Queries for Multiple-Object Tracking.**<br>
*Yihong Xu, Yutong Ban, Guillaume Delorme, Chuang Gan, Daniela Rus, Xavier Alameda-Pineda.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.15145)]

**TFPose: Direct Human Pose Estimation with Transformers.**<br>
*Weian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, Zhibin Wang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.15320)]

**Transformer Tracking.**<br>
*Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, Huchuan Lu.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2103.15436)]

**Vision Transformers for Dense Prediction.**<br>
*René Ranftl, Alexey Bochkovskiy, [Vladlen Koltun](http://vladlen.info/).*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.13413)] [[Github](https://github.com/intel-isl/DPT)]

**Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking.**<br>
*Ning Wang, Wengang Zhou, Jie Wang, Houqaing Li.*<br>
CVPR 2021 (Oral). [[PDF](https://arxiv.org/abs/2103.11681)]

**End-to-End Human Object Interaction Detection with HOI Transformer.**<br>
*Cheng Zou, Bohan Wang, Yue Hu, Junqi Liu, Qian Wu, Yu Zhao, Boxun Li, Chenguang Zhang, Chi Zhang, Yichen Wei, Jian Sun.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2103.04503v1)] [[Github](https://github.com/bbepoch/HoiTransformer)]

**TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation.**<br>
*Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L. Yuille, Yuyin Zhou.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.04306)] [[Github](https://github.com/Beckschen/TransUNet)]

**Segmenting Transparent Object in the Wild with Transformer.**<br>
*Enze Xie, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu, Ding Liang, Ping Luo.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.08461)]

**LSTR: End-to-end Lane Shape Prediction with Transformers.**<br>
*Ruijin Liu, Zejian Yuan, Tie Liu, Zhiliang Xiong.*<br>
WACV 2021. [[PDF](https://arxiv.org/abs/2011.04233)] [[Github](https://github.com/liuruijin17/LSTR)]

**Line Segment Detection Using Transformers without Edges.**<br>
*Yifan Xu, Weijian Xu, David Cheung, Zhuowen Tu.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.01909)]

**SETR: Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers.**<br>
*Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip H.S. Torr, Li Zhang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2012.15840)] [[Project](https://fudan-zvg.github.io/SETR/)]

**TrackFormer: Multi-Object Tracking with Transformers.**<br>
*Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, Christoph Feichtenhofer.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.02702)]

**TransTrack: Multiple-Object Tracking with Transformer.**<br>
*Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao, Xinting Hu, Tao Kong, Zehuan Yuan, Changhu Wang, Ping Luo.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2012.15460)]

**MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers.**<br>
*Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, Liang-Chieh Chen.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.00759)]

**UP-DETR: Unsupervised Pre-training for Object Detection with Transformers.**<br>
*Zhigang Dai, Bolun Cai, Yugeng Lin, Junying Chen.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2011.09094)]

**End-to-End Video Instance Segmentation with Transformers.**<br>
*Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, Huaxia Xia.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2011.14503)]

**Visual Transformers: Token-based Image Representation and Processing for Computer Vision.**<br>
*Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, Peter Vajda.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2006.03677)] [[Github](https://github.com/tahmid0007/VisualTransformers)]

**Deformable DETR: Deformable Transformers for End-to-End Object Detection.**<br>
*Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2010.04159)]

**Topological Planning with Transformers for Vision-and-Language Navigation.**<br>
*Kevin Chen, Junshen K. Chen, Jo Chuang, Marynel Vázquez, Silvio Savarese.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.05292)]

**RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder.**<br>
*Cheng Chi, Fangyun Wei, Han Hu.*<br>
NeurIPS 2020. [[PDF](https://arxiv.org/abs/2010.15831)] [[Github](https://github.com/microsoft/RelationNet2)]

## Generation

**iLAT: The Image Local Autoregressive Transformer.**<br>
*Chenjie Cao, Yuxin Hong, Xiang Li, Chengrong Wang, Chengming Xu, XiangYang Xue, Yanwei Fu.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.02514)]

**StyTr^2: Unbiased Image Style Transfer with Transformers.**<br>
*Yingying Deng, Fan Tang, Xingjia Pan, Weiming Dong, ChongyangMa, Changsheng Xu.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2105.14576)]

**Cloth Interactive Transformer for Virtual Try-On.**<br>
*Bin Ren, Hao Tang, Fanyang Meng, Runwei Ding, Ling Shao, Philip H.S. Torr, Nicu Sebe.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.05519)]

**Variational Transformer Networks for Layout Generation.**<br>
*Diego Martin Arroyo, Janis Postels, Federico Tombari.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2104.02416)]

**Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding.**<br>
*Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, Jianfeng Gao.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.15358)]

**Single-Shot Motion Completion with Transformer.**<br>
*Yinglin Duan, Tianyang Shi, Zhengxia Zou, Yenan Lin, Zhehui Qian, Bohan Zhang, Yi Yuan.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.00776)] [[Project](https://github.com/FuxiCV/SSMCT)]

**Generative Adversarial Transformers.**<br>
*Drew A. Hudson, C. Lawrence Zitnick.*<br>
arxiv 2021. [[PDF](https://arxiv.org/pdf/2103.01209.pdf)]

**IBRNet: Learning Multi-View Image-Based Rendering.**<br>
*Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas Funkhouser.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.13090)]

**Deepfake Video Detection Using Convolutional Vision Transformer.**<br>
*Deressa Wodajo, Solomon Atnafu.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.11126)]

**TransGAN: Two Transformers Can Make One Strong GAN.**<br>
*Yifan Jiang, Shiyu Chang, Zhangyang Wang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.07074)] [[Github](https://github.com/VITA-Group/TransGAN)]

**Transformer for Image Quality Assessment.**<br>
*Junyong You, Jari Korhonen.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.01097)]

**ConvTransformer: A Convolutional Transformer Network for Video Frame Synthesis.**<br>
*Zhouyong Liu, Shun Luo, Wubin Li, Jingben Lu, Yufan Wu, Chunguo Li, Luxi Yang.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2011.10185)]

**General Invertible Transformations for Flow-based Generative Modeling.**<br>
*Jakub M. Tomczak.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2011.15056v1)] [[Github](https://github.com/jmtomczak/git_flow)]

**Taming Transformers for High-Resolution Image Synthesis.**<br>
*[Patrick Esser](https://github.com/pesser), [Robin Rombach](https://github.com/rromb), [Björn Ommer](https://hci.iwr.uni-heidelberg.de/Staff/bommer).*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.09841)] [[Project](https://compvis.github.io/taming-transformers/)] [[Github](https://github.com/CompVis/taming-transformers)]

**DeiT: Training Data-efficient Image Transformers & Distillation through Attention.**<br>
*Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.12877)] [[Github](https://github.com/facebookresearch/deit)]

**Transformer Interpretability Beyond Attention Visualization.**<br>
*Hila Chefer, Shir Gur, Lior Wolf.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2012.09838)]

**TSceneFormer: Indoor Scene Generation with Transformers.**<br>
*Xinpeng Wang, Chandan Yeshwanth, Matthias Nießner.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.09793)]

**Synthesizer: Rethinking Self-Attention in Transformer Models.**<br>
*Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2005.00743)] [[Github](https://github.com/10-zin/Synthesizer)]

**Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network.**<br>
*Jiayi Ji, Yunpeng Luo, Xiaoshuai Sun, Fuhai Chen, Gen Luo, Yongjian Wu, Yue Gao, Rongrong Ji.*<br>
AAAI 2021. [[PDF](https://arxiv.org/abs/2012.07061)]

**Image-GPT: Generative Pretraining from Pixels.**<br>
*Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever.*<br>
Tech Report 2020. [[PDF](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf)] [[Github](https://github.com/openai/image-gpt)]

**Generative Pretraining from Pixels.**<br>
*Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever.*<br>
ICML 2020. [[PDF](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf)] [[Github](https://github.com/openai/image-gpt)]

## Restoration

**Uformer: A General U-Shaped Transformer for Image Restoration.**<br> 
*Zhendong Wang, Xiaodong Cun, Jianmin Bao, Jianzhuang Liu.*<br> 
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.03106)]

**Colorization Transformer.**<br> 
*Manoj Kumar, Dirk Weissenborn, Nal Kalchbrenner.*<br>
ICLR 2021. [[PDF](https://openreview.net/pdf?id=5NA1PinlGFu)]

**TTSR: Learning Texture Transformer Network for Image Super-Resolution.**<br>
*Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, Baining Guo.*<br>
CVPR 2020. [[PDF](https://arxiv.org/abs/2006.04139)] [[Github](https://github.com/researchmm/TTSR)]

**Pre-Trained Image Processing Transformer.**<br>
*Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, Wen Gao.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2012.00364)]

## 2.5D and 3D 

**LegoFormer: Transformers for Block-by-Block Multi-view 3D Reconstruction.**<br>
*Farid Yagubbayli, Alessio Tonioni, Federico Tombari.*<br>
arxiv 2021.	[[PDF](https://arxiv.org/abs/2106.12102)]

**THUNDR: Transformer-based 3D HUmaN Reconstruction with Markers.**<br>
*Mihai Zanfir, Andrei Zanfir, Eduard Gabriel Bazavan, William T. Freeman, Rahul Sukthankar, Cristian Sminchisescu.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.09336)]

**Semantic Correspondence with Transformers.**<br>
*Seokju Cho, Sunghwan Hong, Sangryul Jeon, Yunsung Lee, Kwanghoon Sohn, Seungryong Kim.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.02520)] [[Github](https://github.com/SunghwanHong/CATs)]

**Action-Conditioned 3D Human Motion Synthesis with Transformer VAE.**<br>
*Mathis Petrovich, Michael J. Black, Gül Varol.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.05670)]

**Group-Free 3D Object Detection via Transformers.**<br>
*[Ze Liu](https://github.com/zeliu98), [Zheng Zhang](https://github.com/stupidZZ), [Yue Cao](https://github.com/caoyue10), [Han Hu](https://github.com/ancientmooner), [Xin Tong](http://www.xtong.info/).*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.00678)] [[Github](https://github.com/zeliu98/Group-Free-3D)]

**Multi-view 3D Reconstruction with Transformer.**<br>
*Dan Wang, Xinrui Cui, Xun Chen, Zhengxia Zou, Tianyang Shi, Septimiu Salcudean, Z. Jane Wang, Rabab Ward.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.12957)]

**Transformer Guided Geometry Model for Flow-Based Unsupervised Visual Odometry.**<br>
*Xiangyu Li, Yonghong Hou, Pichao Wang, Zhimin Gao, Mingliang Xu, Wanqing Li.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.02143)]

**Spherical Transformer: Adapting Spherical Signal to Convolutional Networks.**<br>
*Haikuan Du, Hui Cao, Shen Cai, Junchi Yan, Siyu Zhang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.03848)]

**Human Mesh Recovery from Multiple Shots.**<br>
*Georgios Pavlakos, Jitendra Malik, Angjoo Kanazawa.*<br>
arxiv 2021. [[PDF](https://arxiv.org/pdf/2012.09843.pdf)] [[Github](https://geopavlakos.github.io/multishot)]

**PCT: PCT: Point Cloud Transformer.**<br>
*Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R. Martin, Shi-Min Hu.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.09688)] [[Github](https://github.com/MenghaoGuo/PCT)]

**Point Transformer.**<br>
*Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, Vladlen Koltun.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.09164)]

**Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers.**<br>
*Zhaoshuo Li, Xingtong Liu, Francis X. Creighton, Russell H. Taylor, Mathias Unberath.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2011.02910)] [[Github](https://github.com/mli0603/stereo-transformer)]

**TransPose: Towards Explainable Human Pose Estimation by Transformer.**<br>
*Sen Yang, Zhibin Quan, Mu Nie, Wankou Yang.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.14214)]

**End-to-End Human Pose and Mesh Reconstruction with Transformers.**<br>
*Kevin Lin, Lijuan Wang, Zicheng Liu.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.09760)]

## Multi-modality

**Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers.**<br>
*Mandela Patrick, Dylan Campbell, Yuki M. Asano, Ishan Misra Florian Metze, Christoph Feichtenhofer, Andrea Vedaldi, Jo\ão F. Henriques.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.05392)] [[Project](https://facebookresearch.github.io/Motionformer)]

**Space-time Mixing Attention for Video Transformer.**<br>
*Adrian Bulat, Juan-Manuel Perez-Rua, Swathikiran Sudhakaran, Brais Martinez, Georgios Tzimiropoulos.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2106.05968)]

**UFC-BERT: Unifying Multi-Modal Controls for Conditional Image Synthesis.**<br>
*Zhu Zhang, Jianxin Ma, Chang Zhou, Rui Men, Zhikang Li, Ming Ding, Jie Tang, Jingren Zhou, Hongxia Yang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2105.14211)]

**CogView: Mastering Text-to-Image Generation via Transformers.**<br>
*Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, Jie Tang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2105.13290)] [[Github](https://github.com/THUDM/CogView)] [[Demo](https://lab.aminer.cn/cogview/index.html)]

**VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text.**<br>
*Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, Boqing Gong.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.11178)]

**Language-based Video Editing via Multi-Modal Multi-Level Transformer.**<br>
*Tsu-Jui Fu, Xin Eric Wang, Scott T. Grafton, Miguel P. Eckstein, William Yang Wang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.01122)]

**Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers.**<br>
*Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, Andrew Zisserman.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2103.16553)]

**Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers.**<br>
*Hila Chefer, Shir Gur, Lior Wolf.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.15679)] [[Github](https://github.com/hila-chefer/Transformer-MM-Explainability)]

**Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning.**<br>
*Amaia Salvador, Erhan Gundogdu, Loris Bazzani, Michael Donoser.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2103.13061)] [[Github](https://github.com/amzn/image-to-recipe-transformers)]

**DanceNet3D: Music Based Dance Generation with Parametric Motion Transformer.**<br>
*Buyu Li, Yongchi Zhao, Lu Sheng.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.10206)] 

**Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer.**<br>
*[Ronghang Hu](https://ronghanghu.com/), Amanpreet Singh.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.10772)] [[Project](https://mmf.sh/)] [[Github](https://github.com/facebookresearch/mmf)]

**End-to-end Audio-visual Speech Recognition with Conformers.**<br>
*Pingchuan Ma, Stavros Petridis, Maja Pantic.*<br>
ICASSP 2021. [[PDF](https://arxiv.org/abs/2102.06657)]

**ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision.**<br>
*Wonjae Kim, Bokyung Son, Ildoo Kim.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.03334)]

**Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers.**<br>
*Lisa Anne Hendricks, John Mellor, Rosalia Schneider, Jean-Baptiste Alayrac, Aida Nematzadeh.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.00529)]

**CPTR: Full Transformer Network for Image Captioning.**<br>
*Wei Liu, Sihan Chen, Longteng Guo, Xinxin Zhu, Jing Liu.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.10804)]

**Dual-Level Collaborative Transformer for Image Captioning.**<br>
*Yunpeng Luo, Jiayi Ji, Xiaoshuai Sun, Liujuan Cao, Yongjian Wu, Feiyue Huang, Chia-Wen Lin, Rongrong Ji.*<br>
AAAI 2021. [[PDF](https://arxiv.org/abs/2101.06462)]

**DALL·E: Creating Images from Text.**<br>
*OpenAI.*<br> [[Blog](https://openai.com/blog/dall-e/#fn1)] [[Github](https://github.com/lucidrains/DALLE-pytorch)]

**CLIP: Learning Transferable Visual Models From Natural Language Supervision.**<br>
*Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.*<br>
2021. [[PDF](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language.pdf)] [[Github](https://github.com/openai/CLIP)] [[Blog](https://openai.com/blog/clip/)] [[Colab](https://colab.research.google.com/github/tg-bomze/collection-of-notebooks/blob/master/Text2Image_v3.ipynb)]

**VisualSparta: Sparse Transformer Fragment-level Matching for Large-scale Text-to-Image Search.**<br>
*Xiaopeng Lu, Tiancheng Zhao, Kyusong Lee.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.00265)]

**COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning.**<br>
*Simon Ging, Mohammadreza Zolfaghari, Hamed Pirsiavash, Thomas Brox.*<br>
NeurIPS 2020. [[PDF](https://arxiv.org/abs/2011.00597)] [[Github](https://github.com/gingsi/coot-videotext)]

**Entangled Transformer for Image Captioning.**<br>
*Guang Li Linchao Zhu Ping Liu Yi Yang.*<br>
ICCV 2019. [[PDF](https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Entangled_Transformer_for_Image_Captioning_ICCV_2019_paper.pdf)]

## Security

**On the Robustness of Vision Transformers to Adversarial Examples.**<br>
*Kaleel Mahmood, Rigel Mahmood, Marten van Dijk.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.02610)]

**On the Adversarial Robustness of Visual Transformers.**<br>
*Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, Cho-Jui Hsieh.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.15670)]

## Misc

**Intriguing Properties of Vision Transformers.**<br>
*Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2105.10497)] [[Github](https://github.com/Muzammal-Naseer/Intriguing-Properties-of-Vision-Transformers)]

**Multimodal Motion Prediction with Stacked Transformers.**<br>
*Yicheng Liu, Jinghuai Zhang, Liangji Fang, Qinhong Jiang, Bolei Zhou.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2103.11624)] [[Github](https://github.com/decisionforce/mmTransformer)] [[Project](https://decisionforce.github.io/mmTransformer/)]

**UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers.**<br>
*Siyi Hu, Fengda Zhu, Xiaojun Chang, Xiaodan Liang.*<br>
ICLR 2021. [[PDF](https://openreview.net/forum?id=v9c7hr9ADKx)]

**Generative Modelling of BRDF Textures from Flash Images.**<br>
*Philipp Henzler, Valentin Deschaintre, Niloy J. Mitra, Tobias Ritschel.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.11861)]

**AttentionLite: Towards Efficient Self-Attention Models for Vision.**<br>
*Souvik Kundu, Sairam Sundaresan.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2101.05216)]

**Transformer Interpretability Beyond Attention Visualization.**<br>
*Hila Chefer, Shir Gur, Lior Wolf.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.09838)] [[Github](https://github.com/hila-chefer/Transformer-Explainability)]

**FPT: Feature Pyramid Transformer.**<br>
*Dong Zhang, Hanwang Zhang, Jinhui Tang, Meng Wang, Xiansheng Hua, Qianru Sun.*<br>
ECCV 2020. [[PDF](https://arxiv.org/abs/2007.09451)] [[Github](https://github.com/ZHANGDONG-NJUST/FPT)]

**R-Transformer: Recurrent Neural Network Enhanced Transformer.**<br>
*Zhiwei Wang, Yao Ma, Zitao Liu, Jiliang Tang.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/1907.05572)] [[Github](https://github.com/DSE-MSU/R-transformer)]

**Rethinking Attention with Performers.**<br>
*Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2009.14794)] [[Github](https://github.com/google-research/google-research/tree/master/performer)]