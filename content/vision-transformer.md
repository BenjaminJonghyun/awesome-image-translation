<p align=center>`Vision Transformer`</p>

## Improvement

### Long-Range

**LambdaNetworks: Modeling Long-Range Interactions Without Attention.**<br>
*Irwan Bello.*<br>
ICLR 2021. [[PDF](https://arxiv.org/abs/2102.08602)]

**Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention.**<br>
*Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.*<br>
AAAI 2021. [[PDF](https://arxiv.org/abs/2102.03902v1)] [[Github](https://github.com/mlpen/Nystromformer)]

### Postion Encoding

**LocalViT: Bringing Locality to Vision Transformers.**<br>
*Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, Luc Van Gool.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.05707)]

**Do We Really Need Explicit Position Encodings for Vision Transformers?**<br>
*Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, Huaxia Xia.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.10882)]

### Accelerate

**Visual Transformer Pruning.**<br>
*Mingjian Zhu, Kai Han, Yehui Tang, Yunhe Wang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.08500)]

**LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference.**<br>
*Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, Matthijs Douze.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.01136)]

**Transformer in Transformer.**<br>
*Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.00112)] [[Github](https://github.com/huawei-noah/noah-research/tree/master/TNT)]

**Optimizing Inference Performance of Transformers on CPUs.**<br>
*Dave Dice, Alex Kogan.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.06621)]

### Training

**An Empirical Study of Training Self-Supervised Visual Transformers.**<br>
*Xinlei Chen, Saining Xie, Kaiming He.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.02057)]

**SiT: Self-supervised vIsion Transformer.**<br>
*Sara Atito, Muhammad Awais, Josef Kittler.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.03602)]

## Survey

**A Survey on Visual Transformer.**<br>
*Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, Dacheng Tao.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2012.12556)]

**Transformers in Vision: A Survey.**<br>
*Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, Mubarak Shah.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.01169)]

## Recognition and Classification

**Multiscale Vision Transformers.**<br>
*Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, Christoph Feichtenhofer.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.11227)] [[Github](https://github.com/facebookresearch/SlowFast)]

**Face Transformer for Recognition.**<br>
*Yaoyao Zhong, Weihong Deng.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.14803)]

**DeepViT: Towards Deeper Vision Transformer.**<br>
*Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou, Jiashi Fengv.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.11886)]

**ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases.**<br>
*Stéphane d'Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, Levent Sagun.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.10697)]

**TimeSformer: Is Space-Time Attention All You Need for Video Understanding?**<br>
*Gedas Bertasius, Heng Wang, Lorenzo Torresani.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.05095)] [[Github](https://github.com/lucidrains/TimeSformer-pytorch)]

**MIST: Multiple Instance Spatial Transformer Network.**<br>
*Baptiste Angles, Yuhe Jin, Simon Kornblith, Andrea Tagliasacchi, Kwang Moo Yi.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/1811.10725)]

**OmniNet: Omnidirectional Representations from Transformers.**<br>
*Yi Tay, Mostafa Dehghani, Vamsi Aribandi, Jai Gupta, Philip Pham, Zhen Qin, Dara Bahri, Da-Cheng Juan, Donald Metzler.*<br>
arxiv 2021. [[PDF](https://arxiv.org/pdf/2103.01075.pdf)]

**Relaxed Transformer Decoders for Direct Action Proposal Generation.**<br>
*Jing Tan, Jiaqi Tang, Limin Wang, Gangshan Wu.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.01894)]

**Video Transformer Network.**<br>
*Daniel Neimark, Omri Bar, Maya Zohar, Dotan Asselmann.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.00719)

**Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet.**<br>
*Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, Shuicheng Yan.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.11986)] [[Github](https://github.com/yitu-opensource/T2T-ViT)]

**Vision Transformer-An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.**<br>
*Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.*<br>
ICLR 2021. [[PDF](https://arxiv.org/abs/2010.11929)] [[Official](https://github.com/google-research/vision_transformer)] [[Github](https://github.com/lukemelas/PyTorch-Pretrained-ViT)] [[Github](https://github.com/lucidrains/vit-pytorch)] [[Github](https://github.com/gupta-abhay/ViT)] [[Code Collection](https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers)]

**Temporal-Relational CrossTransformers for Few-Shot Action Recognition.**<br>
*Toby Perrett, Alessandro Masullo, Tilo Burghardt, Majid Mirmehdi, Dima Damen.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.06184)]

**Bottleneck Transformers for Visual Recognition.**<br>
*Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, Ashish Vaswani.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.11605)]

## Tracking, Detection and Segmentation

**Multi-Modal Fusion Transformer for End-to-End Autonomous Driving.**<br>
*Aditya Prakash, Kashyap Chitta, Andreas Geiger.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2104.09224)]

**Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.**<br>
*Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.14030)] [[Github](https://github.com/microsoft/Swin-Transformer)]

**Looking Beyond Two Frames: End-to-End Multi-Object Tracking Using Spatial and Temporal Transformers.**<br>
*Tianyu Zhu, Markus Hiller, Mahsa Ehsanpour, Rongkai Ma, Tom Drummond, Hamid Rezatofighi.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.14829)]

**TransCenter: Transformers with Dense Queries for Multiple-Object Tracking.**<br>
*Yihong Xu, Yutong Ban, Guillaume Delorme, Chuang Gan, Daniela Rus, Xavier Alameda-Pineda.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.15145)]

**TFPose: Direct Human Pose Estimation with Transformers.**<br>
*Weian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, Zhibin Wang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.15320)]

**Transformer Tracking.**<br>
*Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, Huchuan Lu.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2103.15436)]

**Vision Transformers for Dense Prediction.**<br>
*René Ranftl, Alexey Bochkovskiy, [Vladlen Koltun](http://vladlen.info/).*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.13413)] [[Github](https://github.com/intel-isl/DPT)]

**Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking.**<br>
*Ning Wang, Wengang Zhou, Jie Wang, Houqaing Li.*<br>
CVPR 2021 (Oral). [[PDF](https://arxiv.org/abs/2103.11681)]

**End-to-End Human Object Interaction Detection with HOI Transformer.**<br>
*Cheng Zou, Bohan Wang, Yue Hu, Junqi Liu, Qian Wu, Yu Zhao, Boxun Li, Chenguang Zhang, Chi Zhang, Yichen Wei, Jian Sun.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2103.04503v1)] [[Github](https://github.com/bbepoch/HoiTransformer)]

**TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation.**<br>
*Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L. Yuille, Yuyin Zhou.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.04306)] [[Github](https://github.com/Beckschen/TransUNet)]

**Segmenting Transparent Object in the Wild with Transformer.**<br>
*Enze Xie, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu, Ding Liang, Ping Luo.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.08461)]

**LSTR: End-to-end Lane Shape Prediction with Transformers.**<br>
*Ruijin Liu, Zejian Yuan, Tie Liu, Zhiliang Xiong.*<br>
WACV 2021. [[PDF](https://arxiv.org/abs/2011.04233)] [[Github](https://github.com/liuruijin17/LSTR)]

**Line Segment Detection Using Transformers without Edges.**<br>
*Yifan Xu, Weijian Xu, David Cheung, Zhuowen Tu.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.01909)]

**SETR: Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers.**<br>
*Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip H.S. Torr, Li Zhang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2012.15840)] [[Project](https://fudan-zvg.github.io/SETR/)]

**TrackFormer: Multi-Object Tracking with Transformers.**<br>
*Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, Christoph Feichtenhofer.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.02702)]

**TransTrack: Multiple-Object Tracking with Transformer.**<br>
*Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao, Xinting Hu, Tao Kong, Zehuan Yuan, Changhu Wang, Ping Luo.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2012.15460)]

**MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers.**<br>
*Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, Liang-Chieh Chen.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.00759)]

**UP-DETR: Unsupervised Pre-training for Object Detection with Transformers.**<br>
*Zhigang Dai, Bolun Cai, Yugeng Lin, Junying Chen.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2011.09094)]

**End-to-End Video Instance Segmentation with Transformers.**<br>
*Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, Huaxia Xia.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2011.14503)]

**Visual Transformers: Token-based Image Representation and Processing for Computer Vision.**<br>
*Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, Peter Vajda.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2006.03677)] [[Github](https://github.com/tahmid0007/VisualTransformers)]

**Deformable DETR: Deformable Transformers for End-to-End Object Detection.**<br>
*Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2010.04159)]

**Topological Planning with Transformers for Vision-and-Language Navigation.**<br>
*Kevin Chen, Junshen K. Chen, Jo Chuang, Marynel Vázquez, Silvio Savarese.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.05292)]

**RelationNet++: Bridging Visual Representations for Object Detection via Transformer Decoder.**<br>
*Cheng Chi, Fangyun Wei, Han Hu.*<br>
NeurIPS 2020. [[PDF](https://arxiv.org/abs/2010.15831)] [[Github](https://github.com/microsoft/RelationNet2)]

## Generation

**Cloth Interactive Transformer for Virtual Try-On.**<br>
*Bin Ren, Hao Tang, Fanyang Meng, Runwei Ding, Ling Shao, Philip H.S. Torr, Nicu Sebe.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.05519)]

**Variational Transformer Networks for Layout Generation.**<br>
*Diego Martin Arroyo, Janis Postels, Federico Tombari.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2104.02416)]

**Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding.**<br>
*Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, Jianfeng Gao.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.15358)]

**Single-Shot Motion Completion with Transformer.**<br>
*Yinglin Duan, Tianyang Shi, Zhengxia Zou, Yenan Lin, Zhehui Qian, Bohan Zhang, Yi Yuan.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.00776)] [[Project](https://github.com/FuxiCV/SSMCT)]

**Generative Adversarial Transformers.**<br>
*Drew A. Hudson, C. Lawrence Zitnick.*<br>
arxiv 2021. [[PDF](https://arxiv.org/pdf/2103.01209.pdf)]

**IBRNet: Learning Multi-View Image-Based Rendering.**<br>
*Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas Funkhouser.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.13090)]

**Deepfake Video Detection Using Convolutional Vision Transformer.**<br>
*Deressa Wodajo, Solomon Atnafu.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.11126)]

**TransGAN: Two Transformers Can Make One Strong GAN.**<br>
*Yifan Jiang, Shiyu Chang, Zhangyang Wang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.07074)] [[Github](https://github.com/VITA-Group/TransGAN)]

**Transformer for Image Quality Assessment.**<br>
*Junyong You, Jari Korhonen.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.01097)]

**ConvTransformer: A Convolutional Transformer Network for Video Frame Synthesis.**<br>
*Zhouyong Liu, Shun Luo, Wubin Li, Jingben Lu, Yufan Wu, Chunguo Li, Luxi Yang.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2011.10185)]

**General Invertible Transformations for Flow-based Generative Modeling.**<br>
*Jakub M. Tomczak.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2011.15056v1)] [[Github](https://github.com/jmtomczak/git_flow)]

**Taming Transformers for High-Resolution Image Synthesis.**<br>
*[Patrick Esser](https://github.com/pesser), [Robin Rombach](https://github.com/rromb), [Björn Ommer](https://hci.iwr.uni-heidelberg.de/Staff/bommer).*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.09841)] [[Project](https://compvis.github.io/taming-transformers/)] [[Github](https://github.com/CompVis/taming-transformers)]

**DeiT: Training Data-efficient Image Transformers & Distillation through Attention.**<br>
*Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervé Jégou.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.12877)] [[Github](https://github.com/facebookresearch/deit)]

**Transformer Interpretability Beyond Attention Visualization.**<br>
*Hila Chefer, Shir Gur, Lior Wolf.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2012.09838)]

**TSceneFormer: Indoor Scene Generation with Transformers.**<br>
*Xinpeng Wang, Chandan Yeshwanth, Matthias Nießner.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.09793)]

**Synthesizer: Rethinking Self-Attention in Transformer Models.**<br>
*Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2005.00743)] [[Github](https://github.com/10-zin/Synthesizer)]

**Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network.**<br>
*Jiayi Ji, Yunpeng Luo, Xiaoshuai Sun, Fuhai Chen, Gen Luo, Yongjian Wu, Yue Gao, Rongrong Ji.*<br>
AAAI 2021. [[PDF](https://arxiv.org/abs/2012.07061)]

**Image-GPT: Generative Pretraining from Pixels.**<br>
*Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever.*<br>
Tech Report 2020. [[PDF](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf)] [[Github](https://github.com/openai/image-gpt)]

**Generative Pretraining from Pixels.**<br>
*Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever.*<br>
ICML 2020. [[PDF](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf)] [[Github](https://github.com/openai/image-gpt)]

## Restoration

**Colorization Transformer.**<br> 
*Manoj Kumar, Dirk Weissenborn, Nal Kalchbrenner.*<br>
ICLR 2021. [[PDF](https://openreview.net/pdf?id=5NA1PinlGFu)]

**TTSR: Learning Texture Transformer Network for Image Super-Resolution.**<br>
*Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, Baining Guo.*<br>
CVPR 2020. [[PDF](https://arxiv.org/abs/2006.04139)] [[Github](https://github.com/researchmm/TTSR)]

**Pre-Trained Image Processing Transformer.**<br>
*Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, Wen Gao.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2012.00364)]

## 3D 

**Action-Conditioned 3D Human Motion Synthesis with Transformer VAE.**<br>
*Mathis Petrovich, Michael J. Black, Gül Varol.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.05670)]

**Group-Free 3D Object Detection via Transformers.**<br>
*[Ze Liu](https://github.com/zeliu98), [Zheng Zhang](https://github.com/stupidZZ), [Yue Cao](https://github.com/caoyue10), [Han Hu](https://github.com/ancientmooner), [Xin Tong](http://www.xtong.info/).*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.00678)] [[Github](https://github.com/zeliu98/Group-Free-3D)]

**Multi-view 3D Reconstruction with Transformer.**<br>
*Dan Wang, Xinrui Cui, Xun Chen, Zhengxia Zou, Tianyang Shi, Septimiu Salcudean, Z. Jane Wang, Rabab Ward.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.12957)]

**Transformer Guided Geometry Model for Flow-Based Unsupervised Visual Odometry.**<br>
*Xiangyu Li, Yonghong Hou, Pichao Wang, Zhimin Gao, Mingliang Xu, Wanqing Li.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.02143)]

**Spherical Transformer: Adapting Spherical Signal to Convolutional Networks.**<br>
*Haikuan Du, Hui Cao, Shen Cai, Junchi Yan, Siyu Zhang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.03848)]

**Human Mesh Recovery from Multiple Shots.**<br>
*Georgios Pavlakos, Jitendra Malik, Angjoo Kanazawa.*<br>
arxiv 2021. [[PDF](https://arxiv.org/pdf/2012.09843.pdf)] [[Github](https://geopavlakos.github.io/multishot)]

**PCT: PCT: Point Cloud Transformer.**<br>
*Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R. Martin, Shi-Min Hu.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.09688)] [[Github](https://github.com/MenghaoGuo/PCT)]

**Point Transformer.**<br>
*Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, Vladlen Koltun.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.09164)]

**Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers.**<br>
*Zhaoshuo Li, Xingtong Liu, Francis X. Creighton, Russell H. Taylor, Mathias Unberath.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2011.02910)] [[Github](https://github.com/mli0603/stereo-transformer)]

**TransPose: Towards Explainable Human Pose Estimation by Transformer.**<br>
*Sen Yang, Zhibin Quan, Mu Nie, Wankou Yang.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.14214)]

**End-to-End Human Pose and Mesh Reconstruction with Transformers.**<br>
*Kevin Lin, Lijuan Wang, Zicheng Liu.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.09760)]

## Multi-modality

**VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text.**<br>
*Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, Boqing Gong.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.11178)]

**Language-based Video Editing via Multi-Modal Multi-Level Transformer.**<br>
*Tsu-Jui Fu, Xin Eric Wang, Scott T. Grafton, Miguel P. Eckstein, William Yang Wang.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.01122)]

**Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers.**<br>
*Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, Andrew Zisserman.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2103.16553)]

**Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers.**<br>
*Hila Chefer, Shir Gur, Lior Wolf.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.15679)] [[Github](https://github.com/hila-chefer/Transformer-MM-Explainability)]

**Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning.**<br>
*Amaia Salvador, Erhan Gundogdu, Loris Bazzani, Michael Donoser.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2103.13061)] [[Github](https://github.com/amzn/image-to-recipe-transformers)]

**DanceNet3D: Music Based Dance Generation with Parametric Motion Transformer.**<br>
*Buyu Li, Yongchi Zhao, Lu Sheng.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.10206)] 

**Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer.**<br>
*[Ronghang Hu](https://ronghanghu.com/), Amanpreet Singh.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.10772)] [[Project](https://mmf.sh/)] [[Github](https://github.com/facebookresearch/mmf)]

**End-to-end Audio-visual Speech Recognition with Conformers.**<br>
*Pingchuan Ma, Stavros Petridis, Maja Pantic.*<br>
ICASSP 2021. [[PDF](https://arxiv.org/abs/2102.06657)]

**ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision.**<br>
*Wonjae Kim, Bokyung Son, Ildoo Kim.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.03334)]

**Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers.**<br>
*Lisa Anne Hendricks, John Mellor, Rosalia Schneider, Jean-Baptiste Alayrac, Aida Nematzadeh.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.00529)]

**CPTR: Full Transformer Network for Image Captioning.**<br>
*Wei Liu, Sihan Chen, Longteng Guo, Xinxin Zhu, Jing Liu.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.10804)]

**Dual-Level Collaborative Transformer for Image Captioning.**<br>
*Yunpeng Luo, Jiayi Ji, Xiaoshuai Sun, Liujuan Cao, Yongjian Wu, Feiyue Huang, Chia-Wen Lin, Rongrong Ji.*<br>
AAAI 2021. [[PDF](https://arxiv.org/abs/2101.06462)]

**DALL·E: Creating Images from Text.**<br>
*OpenAI.*<br> [[Blog](https://openai.com/blog/dall-e/#fn1)] [[Github](https://github.com/lucidrains/DALLE-pytorch)]

**CLIP: Learning Transferable Visual Models From Natural Language Supervision.**<br>
*Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.*<br>
2021. [[PDF](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language.pdf)] [[Github](https://github.com/openai/CLIP)] [[Blog](https://openai.com/blog/clip/)] [[Colab](https://colab.research.google.com/github/tg-bomze/collection-of-notebooks/blob/master/Text2Image_v3.ipynb)]

**VisualSparta: Sparse Transformer Fragment-level Matching for Large-scale Text-to-Image Search.**<br>
*Xiaopeng Lu, Tiancheng Zhao, Kyusong Lee.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2101.00265)]

**COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning.**<br>
*Simon Ging, Mohammadreza Zolfaghari, Hamed Pirsiavash, Thomas Brox.*<br>
NeurIPS 2020. [[PDF](https://arxiv.org/abs/2011.00597)] [[Github](https://github.com/gingsi/coot-videotext)]

**Entangled Transformer for Image Captioning.**<br>
*Guang Li Linchao Zhu Ping Liu Yi Yang.*<br>
ICCV 2019. [[PDF](https://openaccess.thecvf.com/content_ICCV_2019/papers/Li_Entangled_Transformer_for_Image_Captioning_ICCV_2019_paper.pdf)]

## Security

**On the Robustness of Vision Transformers to Adversarial Examples.**<br>
*Kaleel Mahmood, Rigel Mahmood, Marten van Dijk.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2104.02610)]

**On the Adversarial Robustness of Visual Transformers.**<br>
*Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, Cho-Jui Hsieh.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2103.15670)]

## Misc

**Multimodal Motion Prediction with Stacked Transformers.**<br>
*Yicheng Liu, Jinghuai Zhang, Liangji Fang, Qinhong Jiang, Bolei Zhou.*<br>
CVPR 2021. [[PDF](https://arxiv.org/abs/2103.11624)]

**UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers.**<br>
*Siyi Hu, Fengda Zhu, Xiaojun Chang, Xiaodan Liang.*<br>
ICLR 2021. [[PDF](https://openreview.net/forum?id=v9c7hr9ADKx)]

**Generative Modelling of BRDF Textures from Flash Images.**<br>
*Philipp Henzler, Valentin Deschaintre, Niloy J. Mitra, Tobias Ritschel.*<br>
arxiv 2021. [[PDF](https://arxiv.org/abs/2102.11861)]

**AttentionLite: Towards Efficient Self-Attention Models for Vision.**<br>
*Souvik Kundu, Sairam Sundaresan.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2101.05216)]

**Transformer Interpretability Beyond Attention Visualization.**<br>
*Hila Chefer, Shir Gur, Lior Wolf.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2012.09838)] [[Github](https://github.com/hila-chefer/Transformer-Explainability)]

**FPT: Feature Pyramid Transformer.**<br>
*Dong Zhang, Hanwang Zhang, Jinhui Tang, Meng Wang, Xiansheng Hua, Qianru Sun.*<br>
ECCV 2020. [[PDF](https://arxiv.org/abs/2007.09451)] [[Github](https://github.com/ZHANGDONG-NJUST/FPT)]

**R-Transformer: Recurrent Neural Network Enhanced Transformer.**<br>
*Zhiwei Wang, Yao Ma, Zitao Liu, Jiliang Tang.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/1907.05572)] [[Github](https://github.com/DSE-MSU/R-transformer)]

**Rethinking Attention with Performers.**<br>
*Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller.*<br>
arxiv 2020. [[PDF](https://arxiv.org/abs/2009.14794)] [[Github](https://github.com/google-research/google-research/tree/master/performer)]